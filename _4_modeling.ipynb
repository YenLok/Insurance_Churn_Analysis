{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac13a99",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, re, ast, csv, math, gc, random, enum, argparse, json, requests, time  \n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None) # to ensure console display all columns\n",
    "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
    "pd.set_option('display.max_row', 40)\n",
    "plt.style.use('ggplot')\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "dataPath = Path(r'/run/media/yen/MLData/MLProjects/UW_Insurance_Churn_Analysis/data')\n",
    "pickleDataPath = dataPath / 'pickle'\n",
    "dataInputPath = dataPath / 'input'\n",
    "dataWorkingPath = dataPath / 'working'  \n",
    "dataOutputPath = dataPath / 'output'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75401b52",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from catboost import Pool, CatBoostClassifier\n",
    "from sklearn.metrics import recall_score, roc_auc_score, classification_report, precision_recall_curve, auc, accuracy_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from glmnet import LogitNet # https://github.com/civisanalytics/python-glmnet\n",
    "from pygam import LogisticGAM, s, f # https://pygam.readthedocs.io/en/latest/notebooks/tour_of_pygam.html#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import Dataset as lgb_Dataset\n",
    "from lightgbm import train as lgb_train\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def get_lgb_classification_model(data_df, col_target, col_feature): \n",
    "    \n",
    "    data_df_train, data_df_test = train_test_split(data_df, shuffle=True, random_state=100, stratify=data_df[col_target], test_size=0.2)    \n",
    "    train_set = lgb_Dataset(data = data_df_train[col_feature], label = data_df_train[col_target])\n",
    "    val_set = lgb_Dataset(data = data_df_test[col_feature], label = data_df_test[col_target])\n",
    "    params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'n_jobs': 2,\n",
    "                'learning_rate': 0.1,\n",
    "                'verbose': -1,\n",
    "                'seed': 100,                   \n",
    "                }\n",
    "\n",
    "    model = None\n",
    "    model = lgb_train(params, train_set, \n",
    "                      num_boost_round=200, early_stopping_rounds=40, \n",
    "                      valid_sets = [train_set, val_set], verbose_eval=False)        \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def get_df_woe(data_df_bin, col_bin, col_target):\n",
    "    df_woe = pd.DataFrame(columns = ['feature_bin'], data = col_bin)        \n",
    "    df_woe['feature'] = df_woe['feature_bin'].apply(lambda s: '_'.join(s.split('--_')[:-1]))    \n",
    "    df_woe['dist_good'] = 0\n",
    "    df_woe['dist_bad'] = 0    \n",
    "    \n",
    "    for feature in sorted(list(set(df_woe['feature']))):\n",
    "        \n",
    "        idx = df_woe['feature']==feature\n",
    "        feature_bin_list = list(df_woe.loc[idx,'feature_bin'])\n",
    "        df_woe.loc[idx,'dist_good'] = np.sum(np.array(data_df_bin[feature_bin_list])*(np.tile(np.array(data_df_bin[col_target]), (len(feature_bin_list),1)).T==0),axis=0)/np.sum(data_df_bin[col_target]==0)\n",
    "        df_woe.loc[idx,'dist_bad'] = np.sum(np.array(data_df_bin[feature_bin_list])*(np.tile(np.array(data_df_bin[col_target]), (len(feature_bin_list),1)).T!=0),axis=0)/np.sum(data_df_bin[col_target]!=0)\n",
    "              \n",
    "    df_woe['WOE'] = np.log(df_woe['dist_good']/df_woe['dist_bad'])*100\n",
    "    df_woe.loc[np.abs(df_woe['WOE'])==np.inf, 'WOE'] = 0\n",
    "    df_woe.loc[df_woe['WOE'].isna(), 'WOE'] = 0\n",
    "\n",
    "    woe_map_dict = {}\n",
    "    for feature in sorted(list(set(df_woe['feature']))):\n",
    "        idx = df_woe['feature']==feature\n",
    "        woe_map_dict[feature] = {}\n",
    "        for (feature_bin, woe) in zip(df_woe.loc[idx,'feature_bin'], df_woe.loc[idx,'WOE']):\n",
    "            woe_map_dict[feature][woe] = [feature_bin.split('--_')[-1]]\n",
    "\n",
    "    return (df_woe, woe_map_dict)\n",
    "\n",
    "\n",
    "\n",
    "def map_woe_df(data_df_cat, woe_map_dict):\n",
    "    \n",
    "    def map_func(s):\n",
    "        out = 0.0\n",
    "        for k in woe_map_dict[col]:\n",
    "            if s in woe_map_dict[col][k]:\n",
    "                out = k\n",
    "                break        \n",
    "        return out    \n",
    "\n",
    "    data_df_woe = data_df_cat.copy()\n",
    "    \n",
    "    for col in woe_map_dict:\n",
    "        data_df_woe[col] = data_df_woe[col].apply(lambda s: map_func(s))\n",
    "        data_df_woe[col] = data_df_woe[col].astype(np.float64)\n",
    "        \n",
    "    return data_df_woe\n",
    "\n",
    "\n",
    "\n",
    "def get_model_df(data_df, col_id, col_target, col_num, col_cat, feature_method, woe_map_dict=None):\n",
    "\n",
    "    if feature_method == 'one_hot':\n",
    "        col_init = [col_id, col_target] + col_num\n",
    "        model_df = data_df[col_init].copy()\n",
    "        for col in col_cat:\n",
    "            model_df = pd.concat([model_df, pd.get_dummies(data_df[col], prefix=f\"{col}--\")], axis=1)    \n",
    "        col_feature = [c for c in model_df.columns if c not in [col_id, col_target]]\n",
    "\n",
    "    if feature_method == 'woe':\n",
    "        col_init = [col_id, col_target] + col_num\n",
    "        \n",
    "        if woe_map_dict is None:\n",
    "            model_df = data_df[col_init].copy()\n",
    "            for col in col_cat:\n",
    "                model_df = pd.concat([model_df, pd.get_dummies(data_df[col], prefix=f\"{col}--\")], axis=1)    \n",
    "            \n",
    "            col_bin = [c for c in model_df.columns if c not in col_init]\n",
    "            data_df_bin = model_df[[col_id, col_target] + col_bin].copy()\n",
    "            (_, woe_map_dict) = get_df_woe(data_df_bin, col_bin, col_target)\n",
    "            \n",
    "        \n",
    "        data_df_cat = data_df[col_cat].copy()\n",
    "        data_df_woe = map_woe_df(data_df_cat, woe_map_dict)\n",
    "        model_df = pd.concat([data_df[col_init], data_df_woe], axis=1) \n",
    "        col_feature = [c for c in model_df.columns if c not in [col_id, col_target]]\n",
    "\n",
    "    return (model_df, col_feature, woe_map_dict)\n",
    "\n",
    "\n",
    "\n",
    "def fill_na_df(data_df, col_num, col_cat, na_val_num=0, na_val_cat='NA'):\n",
    "    data_df[col_num] = data_df[col_num].fillna(na_val_num)\n",
    "    data_df[col_cat] = data_df[col_cat].fillna(na_val_cat)    \n",
    "    return data_df\n",
    "\n",
    "\n",
    "def get_undersampled_data(data_df, col_num, col_target):\n",
    "    rus = RandomUnderSampler(sampling_strategy='auto', random_state=100)    \n",
    "    df_rus, y_rus = rus.fit_resample(data_df[col_num], data_df[col_target])  \n",
    "    df_rus[col_target] = y_rus.values\n",
    "    return df_rus     \n",
    "\n",
    "\n",
    "def get_oversampled_data(data_df, col_num, col_target):\n",
    "    ros = RandomOverSampler(sampling_strategy='auto', random_state=100)    \n",
    "    df_ros, y_ros = ros.fit_resample(data_df[col_num], data_df[col_target])  \n",
    "    df_ros[col_target] = y_ros.values\n",
    "    return df_ros    \n",
    "\n",
    "\n",
    "def get_undersampled_oversampled_data(data_df, col_num, col_target):\n",
    "    rus = RandomUnderSampler(sampling_strategy='auto', random_state=100)    \n",
    "    df_rus, y_rus = rus.fit_resample(data_df[col_num], data_df[col_target])  \n",
    "    df_rus[col_target] = y_rus.values    \n",
    "\n",
    "    ros = RandomOverSampler(sampling_strategy='auto', random_state=100)    \n",
    "    df_ros, y_ros = ros.fit_resample(data_df[col_num], data_df[col_target])  \n",
    "    df_ros[col_target] = y_ros.values\n",
    "    \n",
    "    df_output = df_rus.append(df_ros)\n",
    "    df_output.reset_index(drop=True, inplace=True)  \n",
    "    \n",
    "    return df_output    \n",
    "\n",
    "\n",
    "def get_classifier_model(data_df, col_target, col_feature, model_type):\n",
    "    model = None\n",
    "    \n",
    "    if model_type  == 'catboost':    \n",
    "        model = CatBoostClassifier(\n",
    "                                   random_seed=100,\n",
    "                                   od_type='Iter', od_wait=20, \n",
    "                                   eval_metric='AUC', \n",
    "                                   verbose = 0,                                                                 \n",
    "                                   fold_len_multiplier=2,   \n",
    "                                   allow_writing_files=False,\n",
    "                                   )   \n",
    " \n",
    "        model.fit(data_df[col_feature], data_df[col_target])\n",
    "\n",
    "\n",
    "    if model_type == 'lgb':   \n",
    "        model = get_lgb_classification_model(data_df, col_target, col_feature)\n",
    "\n",
    "    if model_type == 'glmnet':    \n",
    "        model = LogitNet()\n",
    "        model.fit(data_df[col_feature], data_df[col_target])\n",
    "\n",
    "    if model_type == 'pygam':\n",
    "        model = LogisticGAM(s(0) + s(1))             \n",
    "        lam = np.logspace(-3, 5, 5)\n",
    "        lams = [lam] * 2\n",
    "        model.gridsearch(data_df[col_feature].values, data_df[col_target].values, lam=lams)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def decile_analysis(data_df, col_id, col_target, y_prob, y_true):\n",
    "    decile_df = data_df[[col_id, col_target]].copy()\n",
    "    decile_df['y'] = y_true\n",
    "    decile_df['y_prob'] = y_prob\n",
    "    \n",
    "    base_response_rate = 100*decile_df[decile_df.y == 1].shape[0]/decile_df.shape[0]\n",
    "    decile_df.sort_values(by = 'y_prob', inplace = True, ascending = False)\n",
    "    decile_df.reset_index(inplace = True)\n",
    "    decile_df['decile'] = np.nan\n",
    "    d = int(np.ceil(decile_df.shape[0]/10))\n",
    "    start = 0\n",
    "    end = d\n",
    "    \n",
    "    for i in range(10):\n",
    "        decile_df.loc[start:end, ['decile']] = i + 1\n",
    "        start = start + d\n",
    "        end = end + d\n",
    "        \n",
    "    decile_result_df = pd.crosstab(decile_df['decile'], decile_df['y'])\n",
    "    decile_result_df.columns = ['zero', 'one']\n",
    "    decile_result_df['min_prob'] = decile_df.groupby(by = ['decile']).min()['y_prob']\n",
    "    decile_result_df['max_prob'] = decile_df.groupby(by = ['decile']).max()['y_prob']\n",
    "    decile_result_df['count'] = decile_df.groupby(by = ['decile']).count()['y_prob']\n",
    "    decile_result_df['gain'] = np.round(100*decile_result_df['one']/decile_result_df['one'].sum(), decimals = 2)\n",
    "    decile_result_df['cum_gain'] = np.cumsum(decile_result_df['gain'])\n",
    "    decile_result_df['lift'] = np.round((100*decile_result_df['one']/decile_result_df['count'])/base_response_rate, 2)  \n",
    "    \n",
    "    return decile_result_df\n",
    "\n",
    "\n",
    "def get_validation_results(valid_date, model, model_type, col_id, col_target, col_num, col_cat, col_feature_init, feature_method, \n",
    "                           scaler, reducer, to_scale, woe_map_dict, print_output=False):\n",
    "    \n",
    "    valid_df = pd.read_csv(os.path.join(dataWorkingPath / f\"model_data_{valid_date}.csv\"))\n",
    "    valid_df = fill_na_df(valid_df, col_num, col_cat)  \n",
    "    (valid_df, _, _) = get_model_df(valid_df, col_id, col_target, col_num, col_cat, feature_method, woe_map_dict)\n",
    "\n",
    "    for col in col_feature_init:\n",
    "        if col not in valid_df.columns:\n",
    "            valid_df[col] = 0\n",
    "        \n",
    "    if to_scale:\n",
    "        valid_df[col_feature_init] = scaler.transform(valid_df[col_feature_init])        \n",
    "           \n",
    "    col_feature = col_feature_init.copy()\n",
    "    if reducer is not None:\n",
    "        col_feature_pca = [f'C{int(i)}' for i in np.arange(1,reducer.n_components_+1)]\n",
    "        valid_df = pd.concat([valid_df, pd.DataFrame(reducer.transform(valid_df[col_feature].values), columns=col_feature_pca)], axis=1)\n",
    "        col_feature = col_feature_pca.copy()\n",
    "\n",
    "    if model_type in ['catboost','glmnet']:\n",
    "        y_pred = model.predict(valid_df[col_feature])*1            \n",
    "        y_prob = model.predict_proba(valid_df[col_feature])[:, 1]                 \n",
    "    \n",
    "    elif model_type in ['lgb']:\n",
    "        y_prob = model.predict(valid_df[col_feature])           \n",
    "        y_pred = (y_prob > 0.5)*1             \n",
    "                \n",
    "    elif 'pygam' in model_type.split('-'):\n",
    "        y_pred = model.predict(valid_df[col_feature])*1            \n",
    "        y_prob = model.predict_proba(valid_df[col_feature])  \n",
    "\n",
    "\n",
    "    y_true = valid_df[col_target].values\n",
    "    decile_result_df = decile_analysis(valid_df, col_id, col_target, y_prob, y_true)   \n",
    "    classification_report_ = classification_report(y_true, y_pred, output_dict=True)\n",
    "    result_dict = {\n",
    "        f'{valid_date}_precision': np.round(classification_report_['1']['precision'], 3),\n",
    "        f'{valid_date}_recall': np.round(classification_report_['1']['recall'], 3),        \n",
    "        }\n",
    "    for i in np.arange(1,6):\n",
    "        result_dict[f\"{valid_date}_top_{i}0_gain\"] = np.round(decile_result_df.loc[int(i)]['cum_gain'],2)   \n",
    "\n",
    " \n",
    "    if print_output:\n",
    "        classification_report_ = pd.DataFrame(classification_report(y_true, y_pred, output_dict=True)).transpose().reset_index() \n",
    "        print(f\"## {valid_date}: {model_type}\")\n",
    "        print(classification_report_)\n",
    "        print()\n",
    "        print(decile_result_df)\n",
    "        print()     \n",
    "\n",
    "    return (classification_report_, decile_result_df, result_dict)\n",
    "    \n",
    "\n",
    "def convert_excel_date_to_date(excel_date):\n",
    "    return datetime.fromordinal(datetime(1900, 1, 1).toordinal() + excel_date - 2).date()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1050695",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We apply the following treatment for modeling: <br>\n",
    "\n",
    "1. We encode categorical data via: <br>\n",
    "    a. one_hot: One hot encoding. This is a popular encoding method that typically results in good performance as well as intepretability. <br>\n",
    "    b. woe: Weight of Evidence encoding. Following the application of WoE on the correlation analysis, we will also compare the performance of WoE encoding versus one-hot. <br>\n",
    "\n",
    "\n",
    "2. We apply the following feature selection method: <br>\n",
    "    a. NA: Use all features <br>\n",
    "    \n",
    "    b. fix: Using the result from the analysis in 3_other_data_explorations.ipynb, we use the following features as they seem to have relatively significant different distribution for policies that lapsed versus those that did not:\n",
    "            CLAWBACK_STYLE, SMOKER1, age_life1, pol_tenure\n",
    "\n",
    "    c. vif: Using the result from the correlation analysis in 2_feature_correlation_analysis.ipynb, we use all features except the following features:\n",
    "            BENEFITCODE, PRODCODE, BENESC\n",
    "\n",
    "    d. pca: Apply the principle decomposition to reduce the features to their important components, in an attempt to reduce noise as well as reduce the effects of correlated features.\n",
    "<br>\n",
    "\n",
    "3. We explore the following models: <br>\n",
    "    a. catboost: Boosting tree model. See https://catboost.ai/ <br>\n",
    "    b. lgb: Boosting tree model. See https://lightgbm.readthedocs.io/en/latest/ <br>\n",
    "    c. glmnet: GLM model with regularization. See https://glmnet.stanford.edu/articles/glmnet.html <br>\n",
    "    d. pygam: GAM model. See https://pygam.readthedocs.io/en/latest/ <br>    \n",
    "    \n",
    "   \n",
    "4. Due to the highly imbalanced data set, we apply the following sampling method when training the model: <br>\n",
    "    a. under: Undersampling. <br>\n",
    "    b. over: Oversampling. <br>\n",
    "    c. underover: We combine both under and oversampling, i.e. we carry out both sampling method and concetenate the resulting table.<br>\n",
    "\n",
    "\n",
    "5. Although all of the above models are robust to feature scaling, we apply standard scaling to the dataset. <br>\n",
    "<br>\n",
    "6. As noted in 3_other_data_explorations.ipynb, initial sum assured have a heavy tail distribution, and hence for the GLM and GAM, we will apply log scaling to it. We did not do this for the tree models, as we found that this will decrease their performance. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "654cd14a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# feature_method: one_hot, feature_selection_method: NA\n",
      "model_type: catboost, sampling_type: under\n",
      "model_type: catboost, sampling_type: over\n",
      "model_type: catboost, sampling_type: underover\n",
      "model_type: lgb, sampling_type: under\n",
      "model_type: lgb, sampling_type: over\n",
      "model_type: lgb, sampling_type: underover\n",
      "model_type: glmnet, sampling_type: under\n",
      "model_type: glmnet, sampling_type: over\n",
      "model_type: glmnet, sampling_type: underover\n",
      "model_type: pygam, sampling_type: under\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:00:08 Time:  0:00:08\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:00:04 Time:  0:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:01:27 Time:  0:01:27\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:01:44 Time:  0:01:44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: underover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:02:06 Time:  0:02:06\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:01:53 Time:  0:01:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# feature_method: one_hot, feature_selection_method: fix\n",
      "model_type: catboost, sampling_type: under\n",
      "model_type: catboost, sampling_type: over\n",
      "model_type: catboost, sampling_type: underover\n",
      "model_type: lgb, sampling_type: under\n",
      "model_type: lgb, sampling_type: over\n",
      "model_type: lgb, sampling_type: underover\n",
      "model_type: glmnet, sampling_type: under\n",
      "model_type: glmnet, sampling_type: over\n",
      "model_type: glmnet, sampling_type: underover\n",
      "model_type: pygam, sampling_type: under\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:00:06 Time:  0:00:06\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:00:07 Time:  0:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:02:11 Time:  0:02:11\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:01:24 Time:  0:01:24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: underover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:02:17 Time:  0:02:17\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:02:02 Time:  0:02:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# feature_method: one_hot, feature_selection_method: vif\n",
      "model_type: catboost, sampling_type: under\n",
      "model_type: catboost, sampling_type: over\n",
      "model_type: catboost, sampling_type: underover\n",
      "model_type: lgb, sampling_type: under\n",
      "model_type: lgb, sampling_type: over\n",
      "model_type: lgb, sampling_type: underover\n",
      "model_type: glmnet, sampling_type: under\n",
      "model_type: glmnet, sampling_type: over\n",
      "model_type: glmnet, sampling_type: underover\n",
      "model_type: pygam, sampling_type: under\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:00:09 Time:  0:00:09\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:00:07 Time:  0:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:02:03 Time:  0:02:03\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:01:51 Time:  0:01:51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: underover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:02:13 Time:  0:02:13\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:01:57 Time:  0:01:57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# feature_method: one_hot, feature_selection_method: pca\n",
      "model_type: catboost, sampling_type: under\n",
      "model_type: catboost, sampling_type: over\n",
      "model_type: catboost, sampling_type: underover\n",
      "model_type: lgb, sampling_type: under\n",
      "model_type: lgb, sampling_type: over\n",
      "model_type: lgb, sampling_type: underover\n",
      "model_type: glmnet, sampling_type: under\n",
      "model_type: glmnet, sampling_type: over\n",
      "model_type: glmnet, sampling_type: underover\n",
      "model_type: pygam, sampling_type: under\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:00:08 Time:  0:00:08\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:00:10 Time:  0:00:10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:02:39 Time:  0:02:39\n",
      " 88% (22 of 25) |#####################   | Elapsed Time: 0:04:51 ETA:   0:03:09"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:05:01 Time:  0:05:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: underover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:01:43 Time:  0:01:43\n",
      " 24% (6 of 25) |######                   | Elapsed Time: 0:00:50 ETA:   0:09:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:03:30 Time:  0:03:30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# feature_method: woe, feature_selection_method: NA\n",
      "model_type: catboost, sampling_type: under\n",
      "model_type: catboost, sampling_type: over\n",
      "model_type: catboost, sampling_type: underover\n",
      "model_type: lgb, sampling_type: under\n",
      "model_type: lgb, sampling_type: over\n",
      "model_type: lgb, sampling_type: underover\n",
      "model_type: glmnet, sampling_type: under\n",
      "model_type: glmnet, sampling_type: over\n",
      "model_type: glmnet, sampling_type: underover\n",
      "model_type: pygam, sampling_type: under\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:00:04 Time:  0:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:01:16 Time:  0:01:16\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:01:06 Time:  0:01:06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: underover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:01:23 Time:  0:01:23\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:01:13 Time:  0:01:13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# feature_method: woe, feature_selection_method: fix\n",
      "model_type: catboost, sampling_type: under\n",
      "model_type: catboost, sampling_type: over\n",
      "model_type: catboost, sampling_type: underover\n",
      "model_type: lgb, sampling_type: under\n",
      "model_type: lgb, sampling_type: over\n",
      "model_type: lgb, sampling_type: underover\n",
      "model_type: glmnet, sampling_type: under\n",
      "model_type: glmnet, sampling_type: over\n",
      "model_type: glmnet, sampling_type: underover\n",
      "model_type: pygam, sampling_type: under\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:00:04 Time:  0:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:01:22 Time:  0:01:22\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:01:14 Time:  0:01:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: underover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:01:30 Time:  0:01:30\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:01:19 Time:  0:01:19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# feature_method: woe, feature_selection_method: vif\n",
      "model_type: catboost, sampling_type: under\n",
      "model_type: catboost, sampling_type: over\n",
      "model_type: catboost, sampling_type: underover\n",
      "model_type: lgb, sampling_type: under\n",
      "model_type: lgb, sampling_type: over\n",
      "model_type: lgb, sampling_type: underover\n",
      "model_type: glmnet, sampling_type: under\n",
      "model_type: glmnet, sampling_type: over\n",
      "model_type: glmnet, sampling_type: underover\n",
      "model_type: pygam, sampling_type: under\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:00:03 Time:  0:00:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:01:16 Time:  0:01:16\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:01:06 Time:  0:01:06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: underover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:01:22 Time:  0:01:22\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:01:12 Time:  0:01:12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# feature_method: woe, feature_selection_method: pca\n",
      "model_type: catboost, sampling_type: under\n",
      "model_type: catboost, sampling_type: over\n",
      "model_type: catboost, sampling_type: underover\n",
      "model_type: lgb, sampling_type: under\n",
      "model_type: lgb, sampling_type: over\n",
      "model_type: lgb, sampling_type: underover\n",
      "model_type: glmnet, sampling_type: under\n",
      "model_type: glmnet, sampling_type: over\n",
      "model_type: glmnet, sampling_type: underover\n",
      "model_type: pygam, sampling_type: under\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (25 of 25) |########################| Elapsed Time: 0:00:04 Time:  0:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:01:50 Time:  0:01:50\n",
      "  8% (2 of 25) |##                       | Elapsed Time: 0:01:28 ETA:   0:30:32"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28% (7 of 25) |#######                  | Elapsed Time: 0:04:13 ETA:   0:39:30"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48% (12 of 25) |###########             | Elapsed Time: 0:06:09 ETA:   0:18:12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64% (16 of 25) |###############         | Elapsed Time: 0:06:55 ETA:   0:05:21"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88% (22 of 25) |#####################   | Elapsed Time: 0:08:59 ETA:   0:04:19"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:09:10 Time:  0:09:10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: pygam, sampling_type: underover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:02:19 Time:  0:02:19\n",
      "  8% (2 of 25) |##                       | Elapsed Time: 0:01:40 ETA:   0:34:41"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28% (7 of 25) |#######                  | Elapsed Time: 0:03:35 ETA:   0:27:18"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48% (12 of 25) |###########             | Elapsed Time: 0:05:27 ETA:   0:19:55"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68% (17 of 25) |################        | Elapsed Time: 0:07:50 ETA:   0:12:09"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88% (22 of 25) |#####################   | Elapsed Time: 0:09:38 ETA:   0:04:26"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (25 of 25) |########################| Elapsed Time: 0:09:51 Time:  0:09:51\n"
     ]
    }
   ],
   "source": [
    "feature_method_list = ['one_hot','woe']\n",
    "feature_selection_method_list = ['NA','fix','vif','pca']\n",
    "model_type_list = ['catboost','lgb','glmnet','pygam']\n",
    "sampling_type = ['under','over','underover']\n",
    "\n",
    "model_sampling_type_list = []\n",
    "for model_type in model_type_list:\n",
    "    model_sampling_type_list += [f\"{model_type}-{c}\" for c in sampling_type]\n",
    "\n",
    "to_scale = True\n",
    "col_cat = ['COMM_STYLE','CLAWBACK_STYLE','REGION','BENEFITCODE',\n",
    "           'gender1','SMOKER1','gender2','SMOKER2','occ_class','JOINTLIFE',\n",
    "           'BENESC','PRODCODE','prem_freq','product','rated']\n",
    "\n",
    "col_num = ['initial_sum_assured','POLTERM','UWLDPERML1_MORT','UWLDPERML2_MORT','UWPERMILL1_MORT','UWPERMILL2_MORT','age_life1','age_life2','pol_tenure']\n",
    "col_id = 'id'\n",
    "col_target = 'target'\n",
    "date_list = ['2016-06-30','2017-06-30','2018-06-30']\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for feature_method in feature_method_list:\n",
    "    \n",
    "    for feature_selection_method in feature_selection_method_list:\n",
    "\n",
    "        print(f\"# feature_method: {feature_method}, feature_selection_method: {feature_selection_method}\")\n",
    "\n",
    "\n",
    "        for model_sampling_type in model_sampling_type_list:  \n",
    "            model_type = model_sampling_type.split('-')[0]\n",
    "            sampling_type = model_sampling_type.split('-')[-1]              \n",
    "            print(f\"model_type: {model_type}, sampling_type: {sampling_type}\")\n",
    "\n",
    "            out_dict = {\n",
    "                'model_type': model_type,\n",
    "                'sampling_type': sampling_type,                \n",
    "                'feature_method': feature_method,\n",
    "                'feature_selection_method': feature_selection_method,    \n",
    "                'scale': to_scale,    \n",
    "                }\n",
    "\n",
    "            for train_date, valid_date in zip(date_list[:-1], date_list[1:]):\n",
    "\n",
    "                train_df = pd.read_csv(os.path.join(dataWorkingPath / f\"model_data_{train_date}.csv\"))\n",
    "                train_df = fill_na_df(train_df, col_num, col_cat)  \n",
    "                \n",
    "                if model_type in ['glmnet','pygam']:\n",
    "                    # Apply log1p transformation to initial_sum_assured as it exhibits heavy tail\n",
    "                    train_df['initial_sum_assured'] = np.log1p(train_df['initial_sum_assured'])\n",
    "                    \n",
    "                \n",
    "                (train_df, col_feature, woe_map_dict) = get_model_df(train_df, col_id, col_target, col_num, col_cat, feature_method)\n",
    "        \n",
    "                if 'fix' in feature_selection_method.split('-'):   \n",
    "                    col_to_keep = ['CLAWBACK_STYLE','SMOKER1','age_life1','pol_tenure']\n",
    "                    col_feature = [c for c in col_feature if c.split('--')[0] in col_to_keep]\n",
    "        \n",
    "                if 'vif' in feature_selection_method.split('-'):   \n",
    "                    col_to_rm = ['BENEFITCODE','PRODCODE','BENESC']\n",
    "                    col_feature = [c for c in col_feature if c.split('--')[0] not in col_to_rm]\n",
    "                    \n",
    "                if 'finh' in feature_selection_method.split('-'):    \n",
    "                    col_feature = ypf.get_important_features(train_df, train_df[col_target], col_feature)    \n",
    "        \n",
    "                scaler = None\n",
    "                if to_scale:\n",
    "                    scaler = StandardScaler()\n",
    "                    scaler.fit(train_df[col_feature])\n",
    "                    train_df[col_feature] = scaler.transform(train_df[col_feature])\n",
    "        \n",
    "                reducer = None\n",
    "                col_feature_init = col_feature.copy()\n",
    "                if 'pca' in feature_selection_method.split('-'):  \n",
    "                    reducer = PCA(n_components=0.95, random_state=100)   \n",
    "                    reducer.fit(train_df[col_feature].values)\n",
    "\n",
    "                    col_feature_pca = [f'C{int(i)}' for i in np.arange(1,reducer.n_components_+1)]\n",
    "                    train_df = pd.concat([train_df, pd.DataFrame(reducer.transform(train_df[col_feature].values), columns=col_feature_pca)], axis=1)\n",
    "                    col_feature = col_feature_pca.copy()\n",
    "\n",
    "\n",
    "                if sampling_type == 'under':\n",
    "                    train_df_sampled = get_undersampled_data(train_df, col_feature, col_target)    \n",
    "                    \n",
    "                if sampling_type == 'over':\n",
    "                    train_df_sampled = get_oversampled_data(train_df, col_feature, col_target)                    \n",
    "                    \n",
    "                if sampling_type == 'underover':\n",
    "                    train_df_sampled = get_undersampled_oversampled_data(train_df, col_feature, col_target)                    \n",
    "                    \n",
    "                model = get_classifier_model(train_df_sampled, col_target, col_feature, model_type)\n",
    "        \n",
    "\n",
    "                (classification_report_, decile_result_df, result_dict) = get_validation_results(valid_date, model, model_type, col_id, col_target, col_num, col_cat, col_feature_init, \n",
    "                                                                                                 feature_method, scaler, reducer, to_scale, woe_map_dict)\n",
    "                out_dict.update(result_dict)\n",
    "\n",
    "            result_df = result_df.append(out_dict, ignore_index=True)\n",
    "                 \n",
    "result_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d64d2fd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We rate the performance of the model based on the capture rate of the top quantiles of the predicted model score (gain score), i.e. we look at the say top 10% of the policies with the highest predicted churn score, and check the % of churned policies in the dataset that lies in these policies. Given that the churn percentage of policies in the dataset is less than 10%, an extremely good (but unrealistic) model will be able to capture close to 100% of the policies that churn. <br>\n",
    "\n",
    "The choice of performance measure typically depends on the way we intend to apply the results of the analysis to business use-cases. The reasoning behind the gain score for insurance churn analysis is that the gain score represents the tradeoff between marketing cost and capture rate. For example, suppose that we have decided to give discount to policy owners if they were to renew their policy in the next month, in an attempt to reduce lapse rate. This discount represents a cost to the business, and we should only promote this to policies that have a relatively high lapse probability to reduce cost. By using gain score, we can calculate the tradeoff between the extra premium due to lower lapse rate versus the cost of the discount, to determine the optimal number of policies that we should give discount to. <br>\n",
    "\n",
    "\n",
    "As noted in 1_data_exploration_and_preprocessing.ipynb: <br>\n",
    "1. We will first train the models on Dataset A, and using these models to make predictions on Dataset B, and then evaluate the model performance. <br>\n",
    "2. Then we train the models on Dataset B, and using these models to make predictions on Dataset C, and then evaluate the model performance. <br>\n",
    "3. The overall model performance will be the aggregated performance across the 2 test datasets.\n",
    "\n",
    "From the results below, we make the following observations and conclusions: <br>\n",
    "\n",
    "1. The capture rate of the top 10% is approx 25%.\n",
    "2. Catboost seems to have the best performance, followed by lgb and glmnet. \n",
    "3. LGB model is less sensitive to correlated features, whereas catboost and glmnet benefit from the removal of these features. This is reflected by the observation that both catboost and glmnet performs best using the \"vif feature selection method, whereas lgb performs best using all features. \n",
    "4. Catboost and GLM performs better in smaller sample data sets, where undersampling is prefered., whereas LGB prefers oversampling.\n",
    "5. WoE encoding seems to work well with GLM, whereas one-hot encoding is prefered for the tree models.\n",
    "6. As GAM are complex model, they tend to overfit to the data, hence they only perform well when a small number of strong features are used. This is reflected by the observation that pygam performs best using the \"fix\" feature selection method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e663c45",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_type sampling_type feature_method feature_selection_method  scale  \\\n",
      "0   catboost         under        one_hot                      vif  1.000   \n",
      "1        lgb     underover        one_hot                       NA  1.000   \n",
      "2     glmnet         under            woe                      vif  1.000   \n",
      "3      pygam         under        one_hot                      fix  1.000   \n",
      "\n",
      "   2017-06-30_precision  2017-06-30_recall  2017-06-30_top_10_gain  \\\n",
      "0                 0.143              0.495                  25.370   \n",
      "1                 0.150              0.460                  25.570   \n",
      "2                 0.135              0.484                  22.550   \n",
      "3                 0.120              0.490                  19.580   \n",
      "\n",
      "   2017-06-30_top_20_gain  2017-06-30_top_30_gain  2017-06-30_top_40_gain  \\\n",
      "0                  40.540                  52.680                  62.690   \n",
      "1                  40.670                  52.380                  62.440   \n",
      "2                  38.150                  50.270                  60.470   \n",
      "3                  34.610                  46.750                  56.920   \n",
      "\n",
      "   2017-06-30_top_50_gain  2018-06-30_precision  2018-06-30_recall  \\\n",
      "0                  71.320                 0.047              0.455   \n",
      "1                  70.540                 0.049              0.404   \n",
      "2                  69.180                 0.045              0.455   \n",
      "3                  66.410                 0.042              0.371   \n",
      "\n",
      "   2018-06-30_top_10_gain  2018-06-30_top_20_gain  2018-06-30_top_30_gain  \\\n",
      "0                  25.240                  40.200                  52.850   \n",
      "1                  25.060                  39.530                  51.290   \n",
      "2                  24.280                  38.930                  50.260   \n",
      "3                  20.670                  34.990                  46.670   \n",
      "\n",
      "   2018-06-30_top_40_gain  2018-06-30_top_50_gain   score  \n",
      "0                  63.110                  71.570 126.157  \n",
      "1                  62.010                  71.000 125.287  \n",
      "2                  59.710                  68.060 118.880  \n",
      "3                  57.060                  66.730 106.190  \n"
     ]
    }
   ],
   "source": [
    "cols = [\n",
    "        'model_type','sampling_type','feature_method','feature_selection_method','scale',\n",
    "        '2017-06-30_precision','2017-06-30_recall',\n",
    "        '2017-06-30_top_10_gain','2017-06-30_top_20_gain','2017-06-30_top_30_gain','2017-06-30_top_40_gain','2017-06-30_top_50_gain',\n",
    "        '2018-06-30_precision','2018-06-30_recall',\n",
    "        '2018-06-30_top_10_gain','2018-06-30_top_20_gain','2018-06-30_top_30_gain','2018-06-30_top_40_gain','2018-06-30_top_50_gain',            \n",
    "        ]\n",
    "result_df = result_df[cols]\n",
    "result_df.to_csv(os.path.join(dataOutputPath / f\"result.csv\"), index=False) \n",
    "\n",
    "result_df_f = result_df.copy()\n",
    "result_df_f['score'] = result_df_f[['2017-06-30_top_10_gain','2018-06-30_top_10_gain']].sum(axis=1) + result_df_f[['2017-06-30_top_20_gain','2018-06-30_top_20_gain']].sum(axis=1) / 2 + result_df_f[['2017-06-30_top_30_gain','2018-06-30_top_30_gain']].sum(axis=1) / 3\n",
    "result_df_f.sort_values(['model_type','score'], ascending=[True,False], inplace=True)\n",
    "result_df_f.drop_duplicates('model_type', keep='first', inplace=True)\n",
    "result_df_f.sort_values('score', ascending=False, inplace=True)\n",
    "result_df_f.reset_index(drop=True, inplace=True)\n",
    "print(result_df_f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2bdd86",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Future work: <br>\n",
    "\n",
    "1. It would seem that the current implementation of GAM (via pygam) is not ideal, as in an attempt to generate many features to try to capture inter-dependencies between the features, the model ended up generating lots of noise leading to poor results. Hence, it may be worthwhile to explore approaches to stabilize the model to make it more robust to feature noises. e.g. the regularization implementation of the glmnet is good, as it lowers the impact of the less useful features. A stable GAM should be able to perform at least as well as GLM, given that GLM is a sub-class of GAM.  <br>\n",
    "<br>\n",
    "2. In the above experiment, we mostly just explore modelling using default parameters that typically work well across different datasets. We can further improve the model results by applying hyperparameter finetuning techniques such as Bayesian optimization algorithms, via the hyperopt package http://hyperopt.github.io/hyperopt/ \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}